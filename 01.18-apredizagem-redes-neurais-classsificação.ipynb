{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando o módulo de aprendizado de máquina de Regressão Logística\n",
    "\n",
    "import pickle as pkl                                     # importando o módulo pickle para salvar o modelo em um arquivo\n",
    "from sklearn.neural_network import MLPClassifier         # importando o módulo de rede neural do sklearn \n",
    "from sklearn.metrics import accuracy_score               # importando o módulo de métricas de precisão\n",
    "from sklearn.metrics import confusion_matrix             # importando o módulo de matriz de confusão\n",
    "from sklearn.metrics import classification_report        # importando o módulo de relatório de classificação\n",
    "import seaborn as sns                                    # importando o módulo para visualização de gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leitura da base de dados que ja foram tratadas e salvas\n",
    "\n",
    "with open('./data/census.pkl', 'rb') as f:\n",
    "    X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27676, 108), (27676,), (4885, 108), (4885,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizando o tamnho da base de dados\n",
    "\n",
    "X_census_treinamento.shape, y_census_treinamento.shape, X_census_teste.shape, y_census_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41361893\n",
      "Iteration 2, loss = 0.32816447\n",
      "Iteration 3, loss = 0.31681511\n",
      "Iteration 4, loss = 0.30981030\n",
      "Iteration 5, loss = 0.30459556\n",
      "Iteration 6, loss = 0.30117624\n",
      "Iteration 7, loss = 0.29781684\n",
      "Iteration 8, loss = 0.29563232\n",
      "Iteration 9, loss = 0.29227651\n",
      "Iteration 10, loss = 0.29035390\n",
      "Iteration 11, loss = 0.28755377\n",
      "Iteration 12, loss = 0.28592338\n",
      "Iteration 13, loss = 0.28389349\n",
      "Iteration 14, loss = 0.28161957\n",
      "Iteration 15, loss = 0.28046814\n",
      "Iteration 16, loss = 0.27795046\n",
      "Iteration 17, loss = 0.27652560\n",
      "Iteration 18, loss = 0.27491258\n",
      "Iteration 19, loss = 0.27377091\n",
      "Iteration 20, loss = 0.27108091\n",
      "Iteration 21, loss = 0.27035442\n",
      "Iteration 22, loss = 0.26823109\n",
      "Iteration 23, loss = 0.26648803\n",
      "Iteration 24, loss = 0.26438452\n",
      "Iteration 25, loss = 0.26375565\n",
      "Iteration 26, loss = 0.26283191\n",
      "Iteration 27, loss = 0.26058955\n",
      "Iteration 28, loss = 0.25900067\n",
      "Iteration 29, loss = 0.25747533\n",
      "Iteration 30, loss = 0.25623176\n",
      "Iteration 31, loss = 0.25451557\n",
      "Iteration 32, loss = 0.25367180\n",
      "Iteration 33, loss = 0.25197578\n",
      "Iteration 34, loss = 0.25093427\n",
      "Iteration 35, loss = 0.25022246\n",
      "Iteration 36, loss = 0.24907652\n",
      "Iteration 37, loss = 0.24753680\n",
      "Iteration 38, loss = 0.24760015\n",
      "Iteration 39, loss = 0.24552190\n",
      "Iteration 40, loss = 0.24321354\n",
      "Iteration 41, loss = 0.24385460\n",
      "Iteration 42, loss = 0.24238966\n",
      "Iteration 43, loss = 0.24148627\n",
      "Iteration 44, loss = 0.23952807\n",
      "Iteration 45, loss = 0.23823360\n",
      "Iteration 46, loss = 0.23781348\n",
      "Iteration 47, loss = 0.23651138\n",
      "Iteration 48, loss = 0.23660006\n",
      "Iteration 49, loss = 0.23470408\n",
      "Iteration 50, loss = 0.23375776\n",
      "Iteration 51, loss = 0.23315240\n",
      "Iteration 52, loss = 0.23156484\n",
      "Iteration 53, loss = 0.23121559\n",
      "Iteration 54, loss = 0.23079648\n",
      "Iteration 55, loss = 0.22922366\n",
      "Iteration 56, loss = 0.22830902\n",
      "Iteration 57, loss = 0.22810009\n",
      "Iteration 58, loss = 0.22741982\n",
      "Iteration 59, loss = 0.22622686\n",
      "Iteration 60, loss = 0.22596303\n",
      "Iteration 61, loss = 0.22504762\n",
      "Iteration 62, loss = 0.22414067\n",
      "Iteration 63, loss = 0.22276146\n",
      "Iteration 64, loss = 0.22231148\n",
      "Iteration 65, loss = 0.22243810\n",
      "Iteration 66, loss = 0.22085094\n",
      "Iteration 67, loss = 0.22082858\n",
      "Iteration 68, loss = 0.21963277\n",
      "Iteration 69, loss = 0.21841464\n",
      "Iteration 70, loss = 0.21868052\n",
      "Iteration 71, loss = 0.21696272\n",
      "Iteration 72, loss = 0.21751911\n",
      "Iteration 73, loss = 0.21501484\n",
      "Iteration 74, loss = 0.21609251\n",
      "Iteration 75, loss = 0.21452676\n",
      "Iteration 76, loss = 0.21402789\n",
      "Iteration 77, loss = 0.21392720\n",
      "Iteration 78, loss = 0.21273523\n",
      "Iteration 79, loss = 0.21203187\n",
      "Iteration 80, loss = 0.21226947\n",
      "Iteration 81, loss = 0.21180337\n",
      "Iteration 82, loss = 0.20932794\n",
      "Iteration 83, loss = 0.21034037\n",
      "Iteration 84, loss = 0.20819921\n",
      "Iteration 85, loss = 0.20880880\n",
      "Iteration 86, loss = 0.20881042\n",
      "Iteration 87, loss = 0.20797495\n",
      "Iteration 88, loss = 0.20704191\n",
      "Iteration 89, loss = 0.20655873\n",
      "Iteration 90, loss = 0.20564388\n",
      "Iteration 91, loss = 0.20469214\n",
      "Iteration 92, loss = 0.20493494\n",
      "Iteration 93, loss = 0.20424050\n",
      "Iteration 94, loss = 0.20269056\n",
      "Iteration 95, loss = 0.20333942\n",
      "Iteration 96, loss = 0.20439014\n",
      "Iteration 97, loss = 0.20083766\n",
      "Iteration 98, loss = 0.20119865\n",
      "Iteration 99, loss = 0.20137233\n",
      "Iteration 100, loss = 0.19961886\n",
      "Iteration 101, loss = 0.19980067\n",
      "Iteration 102, loss = 0.19995138\n",
      "Iteration 103, loss = 0.19930446\n",
      "Iteration 104, loss = 0.19833461\n",
      "Iteration 105, loss = 0.19753076\n",
      "Iteration 106, loss = 0.19677584\n",
      "Iteration 107, loss = 0.19661423\n",
      "Iteration 108, loss = 0.19726957\n",
      "Iteration 109, loss = 0.19642657\n",
      "Iteration 110, loss = 0.19478553\n",
      "Iteration 111, loss = 0.19499140\n",
      "Iteration 112, loss = 0.19455610\n",
      "Iteration 113, loss = 0.19458220\n",
      "Iteration 114, loss = 0.19315815\n",
      "Iteration 115, loss = 0.19449694\n",
      "Iteration 116, loss = 0.19250876\n",
      "Iteration 117, loss = 0.19183885\n",
      "Iteration 118, loss = 0.19227695\n",
      "Iteration 119, loss = 0.19157119\n",
      "Iteration 120, loss = 0.19167817\n",
      "Iteration 121, loss = 0.19085411\n",
      "Iteration 122, loss = 0.18931553\n",
      "Iteration 123, loss = 0.18893082\n",
      "Iteration 124, loss = 0.18965822\n",
      "Iteration 125, loss = 0.18907146\n",
      "Iteration 126, loss = 0.18857669\n",
      "Iteration 127, loss = 0.18867111\n",
      "Iteration 128, loss = 0.18744302\n",
      "Iteration 129, loss = 0.18738786\n",
      "Iteration 130, loss = 0.18799530\n",
      "Iteration 131, loss = 0.18847641\n",
      "Iteration 132, loss = 0.18629076\n",
      "Iteration 133, loss = 0.18616296\n",
      "Iteration 134, loss = 0.18533422\n",
      "Iteration 135, loss = 0.18454989\n",
      "Iteration 136, loss = 0.18569954\n",
      "Iteration 137, loss = 0.18428330\n",
      "Iteration 138, loss = 0.18459098\n",
      "Iteration 139, loss = 0.18365720\n",
      "Iteration 140, loss = 0.18466904\n",
      "Iteration 141, loss = 0.18397302\n",
      "Iteration 142, loss = 0.18403589\n",
      "Iteration 143, loss = 0.18231596\n",
      "Iteration 144, loss = 0.18140171\n",
      "Iteration 145, loss = 0.18044132\n",
      "Iteration 146, loss = 0.18213249\n",
      "Iteration 147, loss = 0.18047713\n",
      "Iteration 148, loss = 0.18021767\n",
      "Iteration 149, loss = 0.17885838\n",
      "Iteration 150, loss = 0.18037819\n",
      "Iteration 151, loss = 0.18094397\n",
      "Iteration 152, loss = 0.18015157\n",
      "Iteration 153, loss = 0.17838415\n",
      "Iteration 154, loss = 0.17816296\n",
      "Iteration 155, loss = 0.17804891\n",
      "Iteration 156, loss = 0.17830163\n",
      "Iteration 157, loss = 0.17794137\n",
      "Iteration 158, loss = 0.17746675\n",
      "Iteration 159, loss = 0.17682548\n",
      "Iteration 160, loss = 0.17713920\n",
      "Iteration 161, loss = 0.17613507\n",
      "Iteration 162, loss = 0.17623758\n",
      "Iteration 163, loss = 0.17603766\n",
      "Iteration 164, loss = 0.17500097\n",
      "Iteration 165, loss = 0.17525055\n",
      "Iteration 166, loss = 0.17563525\n",
      "Iteration 167, loss = 0.17546465\n",
      "Iteration 168, loss = 0.17512667\n",
      "Iteration 169, loss = 0.17471730\n",
      "Iteration 170, loss = 0.17374405\n",
      "Iteration 171, loss = 0.17268465\n",
      "Iteration 172, loss = 0.17341886\n",
      "Iteration 173, loss = 0.17437344\n",
      "Iteration 174, loss = 0.17412287\n",
      "Iteration 175, loss = 0.17122926\n",
      "Iteration 176, loss = 0.17177722\n",
      "Iteration 177, loss = 0.17218865\n",
      "Iteration 178, loss = 0.17083860\n",
      "Iteration 179, loss = 0.17040015\n",
      "Iteration 180, loss = 0.17130072\n",
      "Iteration 181, loss = 0.17053241\n",
      "Iteration 182, loss = 0.17189561\n",
      "Iteration 183, loss = 0.17094652\n",
      "Iteration 184, loss = 0.16907627\n",
      "Iteration 185, loss = 0.17039633\n",
      "Iteration 186, loss = 0.16914602\n",
      "Iteration 187, loss = 0.16963222\n",
      "Iteration 188, loss = 0.16929504\n",
      "Iteration 189, loss = 0.16816493\n",
      "Iteration 190, loss = 0.16880537\n",
      "Iteration 191, loss = 0.16843731\n",
      "Iteration 192, loss = 0.16703846\n",
      "Iteration 193, loss = 0.16922202\n",
      "Iteration 194, loss = 0.16639321\n",
      "Iteration 195, loss = 0.16746782\n",
      "Iteration 196, loss = 0.16684463\n",
      "Iteration 197, loss = 0.16791904\n",
      "Iteration 198, loss = 0.16697960\n",
      "Iteration 199, loss = 0.16558016\n",
      "Iteration 200, loss = 0.16621209\n",
      "Iteration 201, loss = 0.16593857\n",
      "Iteration 202, loss = 0.16435011\n",
      "Iteration 203, loss = 0.16619928\n",
      "Iteration 204, loss = 0.16471541\n",
      "Iteration 205, loss = 0.16415522\n",
      "Iteration 206, loss = 0.16485279\n",
      "Iteration 207, loss = 0.16403397\n",
      "Iteration 208, loss = 0.16551960\n",
      "Iteration 209, loss = 0.16266517\n",
      "Iteration 210, loss = 0.16404054\n",
      "Iteration 211, loss = 0.16366399\n",
      "Iteration 212, loss = 0.16359701\n",
      "Iteration 213, loss = 0.16407283\n",
      "Iteration 214, loss = 0.16487880\n",
      "Iteration 215, loss = 0.16470666\n",
      "Iteration 216, loss = 0.16459558\n",
      "Iteration 217, loss = 0.16264545\n",
      "Iteration 218, loss = 0.16264497\n",
      "Iteration 219, loss = 0.16301219\n",
      "Iteration 220, loss = 0.16257353\n",
      "Iteration 221, loss = 0.16248996\n",
      "Iteration 222, loss = 0.16032252\n",
      "Iteration 223, loss = 0.16119808\n",
      "Iteration 224, loss = 0.16121353\n",
      "Iteration 225, loss = 0.16040514\n",
      "Iteration 226, loss = 0.16070196\n",
      "Iteration 227, loss = 0.16012368\n",
      "Iteration 228, loss = 0.16034810\n",
      "Iteration 229, loss = 0.15982247\n",
      "Iteration 230, loss = 0.16035929\n",
      "Iteration 231, loss = 0.16028469\n",
      "Iteration 232, loss = 0.15940075\n",
      "Iteration 233, loss = 0.16089862\n",
      "Iteration 234, loss = 0.16059745\n",
      "Iteration 235, loss = 0.16044730\n",
      "Iteration 236, loss = 0.15826678\n",
      "Iteration 237, loss = 0.15880824\n",
      "Iteration 238, loss = 0.15831638\n",
      "Iteration 239, loss = 0.15793772\n",
      "Iteration 240, loss = 0.15850550\n",
      "Iteration 241, loss = 0.15690452\n",
      "Iteration 242, loss = 0.15901312\n",
      "Iteration 243, loss = 0.15699327\n",
      "Iteration 244, loss = 0.15756602\n",
      "Iteration 245, loss = 0.15650070\n",
      "Iteration 246, loss = 0.15617344\n",
      "Iteration 247, loss = 0.15633494\n",
      "Iteration 248, loss = 0.15658719\n",
      "Iteration 249, loss = 0.15792946\n",
      "Iteration 250, loss = 0.15638322\n",
      "Iteration 251, loss = 0.15842964\n",
      "Iteration 252, loss = 0.15600326\n",
      "Iteration 253, loss = 0.15662359\n",
      "Iteration 254, loss = 0.15520234\n",
      "Iteration 255, loss = 0.15578093\n",
      "Iteration 256, loss = 0.15505451\n",
      "Iteration 257, loss = 0.15809098\n",
      "Iteration 258, loss = 0.15540566\n",
      "Iteration 259, loss = 0.15464543\n",
      "Iteration 260, loss = 0.15494624\n",
      "Iteration 261, loss = 0.15493300\n",
      "Iteration 262, loss = 0.15488190\n",
      "Iteration 263, loss = 0.15313862\n",
      "Iteration 264, loss = 0.15381541\n",
      "Iteration 265, loss = 0.15318112\n",
      "Iteration 266, loss = 0.15388124\n",
      "Iteration 267, loss = 0.15400166\n",
      "Iteration 268, loss = 0.15456185\n",
      "Iteration 269, loss = 0.15454366\n",
      "Iteration 270, loss = 0.15404633\n",
      "Iteration 271, loss = 0.15288870\n",
      "Iteration 272, loss = 0.15282994\n",
      "Iteration 273, loss = 0.15207667\n",
      "Iteration 274, loss = 0.15298944\n",
      "Iteration 275, loss = 0.15243807\n",
      "Iteration 276, loss = 0.15443891\n",
      "Iteration 277, loss = 0.15262136\n",
      "Iteration 278, loss = 0.15224124\n",
      "Iteration 279, loss = 0.15284753\n",
      "Iteration 280, loss = 0.15193157\n",
      "Iteration 281, loss = 0.15035807\n",
      "Iteration 282, loss = 0.15060336\n",
      "Iteration 283, loss = 0.15161412\n",
      "Iteration 284, loss = 0.15123529\n",
      "Iteration 285, loss = 0.15167277\n",
      "Iteration 286, loss = 0.15057068\n",
      "Iteration 287, loss = 0.15031977\n",
      "Iteration 288, loss = 0.15128677\n",
      "Iteration 289, loss = 0.14990562\n",
      "Iteration 290, loss = 0.15120925\n",
      "Iteration 291, loss = 0.15151098\n",
      "Iteration 292, loss = 0.15222931\n",
      "Iteration 293, loss = 0.15036228\n",
      "Iteration 294, loss = 0.14929319\n",
      "Iteration 295, loss = 0.14991570\n",
      "Iteration 296, loss = 0.14893808\n",
      "Iteration 297, loss = 0.14805166\n",
      "Iteration 298, loss = 0.14825116\n",
      "Iteration 299, loss = 0.14933631\n",
      "Iteration 300, loss = 0.14851455\n",
      "Iteration 301, loss = 0.14866751\n",
      "Iteration 302, loss = 0.14916705\n",
      "Iteration 303, loss = 0.14704754\n",
      "Iteration 304, loss = 0.14782491\n",
      "Iteration 305, loss = 0.14803484\n",
      "Iteration 306, loss = 0.14699320\n",
      "Iteration 307, loss = 0.14703203\n",
      "Iteration 308, loss = 0.14737653\n",
      "Iteration 309, loss = 0.14674703\n",
      "Iteration 310, loss = 0.14797013\n",
      "Iteration 311, loss = 0.14735756\n",
      "Iteration 312, loss = 0.14753993\n",
      "Iteration 313, loss = 0.14752316\n",
      "Iteration 314, loss = 0.14761280\n",
      "Iteration 315, loss = 0.14948231\n",
      "Iteration 316, loss = 0.14700633\n",
      "Iteration 317, loss = 0.14928089\n",
      "Iteration 318, loss = 0.14654263\n",
      "Iteration 319, loss = 0.14724242\n",
      "Iteration 320, loss = 0.14620927\n",
      "Iteration 321, loss = 0.14452045\n",
      "Iteration 322, loss = 0.14544049\n",
      "Iteration 323, loss = 0.14711876\n",
      "Iteration 324, loss = 0.14481464\n",
      "Iteration 325, loss = 0.14485936\n",
      "Iteration 326, loss = 0.14459025\n",
      "Iteration 327, loss = 0.14520531\n",
      "Iteration 328, loss = 0.14527108\n",
      "Iteration 329, loss = 0.14520543\n",
      "Iteration 330, loss = 0.14374691\n",
      "Iteration 331, loss = 0.14454201\n",
      "Iteration 332, loss = 0.14465548\n",
      "Iteration 333, loss = 0.14476328\n",
      "Iteration 334, loss = 0.14689509\n",
      "Iteration 335, loss = 0.14504729\n",
      "Iteration 336, loss = 0.14303217\n",
      "Iteration 337, loss = 0.14282924\n",
      "Iteration 338, loss = 0.14524419\n",
      "Iteration 339, loss = 0.14380311\n",
      "Iteration 340, loss = 0.14324035\n",
      "Iteration 341, loss = 0.14366547\n",
      "Iteration 342, loss = 0.14306201\n",
      "Iteration 343, loss = 0.14237472\n",
      "Iteration 344, loss = 0.14432174\n",
      "Iteration 345, loss = 0.14274727\n",
      "Iteration 346, loss = 0.14250217\n",
      "Iteration 347, loss = 0.14293335\n",
      "Iteration 348, loss = 0.14343795\n",
      "Iteration 349, loss = 0.14083156\n",
      "Iteration 350, loss = 0.14268370\n",
      "Iteration 351, loss = 0.14356710\n",
      "Iteration 352, loss = 0.14077156\n",
      "Iteration 353, loss = 0.14246740\n",
      "Iteration 354, loss = 0.14264432\n",
      "Iteration 355, loss = 0.14348282\n",
      "Iteration 356, loss = 0.14430259\n",
      "Iteration 357, loss = 0.14221255\n",
      "Iteration 358, loss = 0.14265675\n",
      "Iteration 359, loss = 0.14159039\n",
      "Iteration 360, loss = 0.14006005\n",
      "Iteration 361, loss = 0.14038355\n",
      "Iteration 362, loss = 0.14007661\n",
      "Iteration 363, loss = 0.14259785\n",
      "Iteration 364, loss = 0.14061087\n",
      "Iteration 365, loss = 0.14230138\n",
      "Iteration 366, loss = 0.14014133\n",
      "Iteration 367, loss = 0.14050520\n",
      "Iteration 368, loss = 0.14089338\n",
      "Iteration 369, loss = 0.14215096\n",
      "Iteration 370, loss = 0.14064656\n",
      "Iteration 371, loss = 0.14246323\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criando o modelo de rede neural e treinando\n",
    "# modelo -> 3entrada -> 2oculta -> 2oculta -> 1saida \n",
    "\n",
    "nn_census = MLPClassifier( max_iter=1500,                 # numero máximo de iterações\n",
    "                           verbose=True,                  # mostrar o processo de treinamento\n",
    "                           tol=0.00001,                   # tolerância para parada\n",
    "                           hidden_layer_sizes=(55, 55)    # tamanho das camadas ocultas\n",
    "                          )\n",
    "\n",
    "nn_census.fit(X_census_treinamento, y_census_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previsão do modelo de rede neural\n",
    "\n",
    "previsoes_nn = nn_census.predict(X_census_teste)\n",
    "#previsoes_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8149437052200614"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avaliando o modelo com a métrica de acurácia \n",
    "\n",
    "accuracy_score(y_census_teste, previsoes_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# avaliando o modelo com a matriz de confusão com heatmap do seaborn\n",
    "\n",
    "cnn = confusion_matrix(y_census_teste, previsoes_nn)\n",
    "sns.heatmap(cnn, annot=True, fmt='d', cmap='Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.88      0.88      3693\n",
      "        >50K       0.62      0.61      0.62      1192\n",
      "\n",
      "    accuracy                           0.81      4885\n",
      "   macro avg       0.75      0.74      0.75      4885\n",
      "weighted avg       0.81      0.81      0.81      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# avaliando o modelo com o relatório de classificação\n",
    "\n",
    "print(classification_report(y_census_teste, previsoes_nn))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
